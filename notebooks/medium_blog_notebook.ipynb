{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = r'Your document path'\n",
    "api_key =\"Your API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector database and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ai21.embeddings import AI21Embeddings\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(doc_path)\n",
    "doc_pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=10)\n",
    "all_splits = text_splitter.split_documents(doc_pages)\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=all_splits,embedding=AI21Embeddings(api_key=api_key))\n",
    "\n",
    "retriever = vector_store.as_retriever(k=2)\n",
    "\n",
    "#Function to retrive top k documents for a user query\n",
    "def doc_retriver(retriever,query=''):\n",
    "    retriever = vector_store.as_retriever(k=3)\n",
    "    docs=retriever.invoke(query)\n",
    "    context = docs\n",
    "    docs=[]\n",
    "    for doc in context:\n",
    "        docs.append(doc.page_content)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that answers the question when you know the answerfrom the context, humbly reply that you do not know the answer if you have no information in the context related to the question.'), HumanMessage(content=''), HumanMessage(content=''), HumanMessage(content=\"['rocket?']\")])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt_ = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that answers the question when you know the answerfrom the context, humbly reply that you do not know the answer if you have no information in the context related to the question.\"),\n",
    "        MessagesPlaceholder(variable_name='history',optional=True),\n",
    "        MessagesPlaceholder(variable_name='context',optional=True),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(prompt_template,history=[''],context=[''],query=['rocket?']):\n",
    "    \n",
    "    prompt = prompt_template.invoke({\n",
    "            'history':history,\n",
    "            'context':context,\n",
    "            'query':query\n",
    "        })\n",
    "    return prompt\n",
    "\n",
    "format_prompt(prompt_template=prompt_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "def add_to_history(chat_history_demo,type='',message=''):\n",
    "    if type=='user':\n",
    "        chat_history_demo.add_user_message(message=message)\n",
    "    elif type=='ai':\n",
    "        chat_history_demo.add_ai_message(message=message)\n",
    "    return chat_history_demo\n",
    "\n",
    "\n",
    "# Function to generate summary of the chat history using the chat model\n",
    "def chat_summariser(chat_history,chat_model):\n",
    "    memory = ConversationSummaryMemory.from_messages(\n",
    "    llm=chat_model,\n",
    "    chat_memory=chat_history,\n",
    "    return_messages=True\n",
    "    )\n",
    "    summary = [memory.buffer]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, Welcome !!!\n",
      "\n",
      " ******************************************************************************** \n",
      "\n",
      "User :  what are engine oil properties? \n",
      "\n",
      "CHAT BOT :  Engine oil properties include viscosity, thermal stability, detergency, dispersancy, anti-wear, corrosion protection, and foaming tendency. Engine oil also enhances the performance of the engine by reducing friction and to reduce wear. It also cleans, inhibits corrosion, improves sealing, and cools the engine by carrying heat away from moving parts. Engine oils should not enhance the properties of the fuel nor energize the combustion.\n",
      "\n",
      " ******************************************************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_ai21 import ChatAI21\n",
    "\n",
    "chat_model = ChatAI21(model=\"j2-ultra\",api_key=\"Your API KEY\",temperature=0.2,max_tokens=500)\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "initiate_flag=True\n",
    "\n",
    "while True:\n",
    "    if initiate_flag:\n",
    "        print('Hi, Welcome !!!')\n",
    "        print('\\n','*'*80,'\\n')\n",
    "        initiate_flag=False\n",
    "    else:\n",
    "        query = input()\n",
    "        if query.lower()==\"stop\":\n",
    "            break\n",
    "        print('User : ',query,'\\n')\n",
    "        chat_history = add_to_history(chat_history_demo=chat_history,type='user',message=query)\n",
    "        context = doc_retriver(retriever=retriever,query=query)\n",
    "        \n",
    "        if len(chat_history.messages)>4:\n",
    "            summary = chat_summariser(chat_model=chat_model,chat_history=chat_history)\n",
    "            prompt = format_prompt(prompt_template=prompt_,history=summary,context=context,query=query)\n",
    "        else:\n",
    "            prompt = format_prompt(prompt_template=prompt_,history=chat_history.messages,context=context,query=query)\n",
    "        \n",
    "        response = chat_model(prompt.messages)\n",
    "\n",
    "        print('CHAT BOT : ',response.content)\n",
    "        print('\\n','*'*80,'\\n')\n",
    "        chat_history = add_to_history(chat_history_demo=chat_history,type='ai',message=response)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_data =pd.read_csv('Path to ground truth data')\n",
    "\n",
    "quesiton_generating_prompt_ = \"\"\"\n",
    "You are a question generator designed to create questions based on provided context. Follow these guidelines to generate questions:\n",
    "Context: You will be given a paragraph of text, scenario, or relevant information.\n",
    "Question Requirements: should include specific keywords, target a certain aspect, be clear and unambiguous\n",
    "\n",
    "Generate a question for below context :\n",
    "{context_}\"\"\"\n",
    "\n",
    "answer_generating_prompt_ = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that answers the question when you know the answerfrom the context, humbly reply that you do not know the answer if you have no information in the context related to the question.\"),\n",
    "        MessagesPlaceholder(variable_name='context',optional=True),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def create_ragas_dataset(quesiton_generating_prompt_,answer_generating_prompt,golden_truth_data):\n",
    "    rag_dataset=[]\n",
    "    for truth in golden_truth_data.ground_truth.tolist():\n",
    "        quesiton_generating_prompt=quesiton_generating_prompt_.format(context_=truth) \n",
    "        question = chat_model.invoke(quesiton_generating_prompt).content\n",
    "        context = doc_retriver(retriever=retriever,query=question)\n",
    "        prompt = format_prompt(prompt_template=answer_generating_prompt,context=context,query=question)\n",
    "        response = chat_model(prompt.messages).content\n",
    "        rag_dataset.append({\n",
    "            \"question\":question,\n",
    "            \"answer\":response,\n",
    "            \"contexts\":context,\n",
    "            \"ground_truth\":truth\n",
    "        })\n",
    "        rag_df=pd.DataFrame(rag_dataset)\n",
    "        eval_dataset = Dataset.from_pandas(rag_df)\n",
    "    return eval_dataset,rag_df\n",
    "eval_dataset = create_ragas_dataset(quesiton_generating_prompt_=quesiton_generating_prompt_,\n",
    "                                    answer_generating_prompt=answer_generating_prompt_,golden_truth_data=ground_truth_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (answer_relevancy,faithfulness,context_recall,context_precision)\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "#Use a different model to create the dataset\n",
    "evaluation_chat_model = LangchainLLMWrapper(ChatAI21(model=\"Model Name\",api_key=\"Your API KEY\",max_tokens=500))\n",
    "evaluation_embeddings = LangchainEmbeddingsWrapper(AI21Embeddings(api_key=\"Your API KEY\"))\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "    result = evaluate(ragas_dataset,metrics=[context_precision,faithfulness,answer_relevancy,context_recall],\n",
    "                      llm=evaluation_chat_model,embeddings=evaluation_embeddings)\n",
    "    return result\n",
    "\n",
    "\n",
    "result = evaluate_ragas_dataset(eval_dataset)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
